# model
model: "huggingface"

# training budget
max_train_seqs: 128 # max number of training samples to use
do_eval: False
max_val_seqs: 0
gpus: 1 # number of gpus to use
train_workers: 2 # number of workers for train_dataloader

# other training config
use_amp: True
use_checkpointing: False

# data
train_path: "/content/drive/My Drive/webtext_train.bin"
val_path: "/content/drive/My Drive/webtext_val.bin"
tokenizer_path: "/content/cramBERT/webtext/tokenizer.json"
micro_batch_size: 32 # 128 or 256 whatever fits in memory
max_batch_size: 32 # recommended 4096
anneal_batch_size: False # whether to anneal batch size
batch_size_anneal_frac: 0 # what fraction of training examples to use for batch-size annealing

# one-cycle lr schedule
pct_start: 0.5
max_lr: 3.0e-4
start_div_factor: 1.0e+4
end_div_factor: 2.5e+4

# optimizer
optimizer: AdamW
b1: 0.9
b2: 0.995
weight_decay: 1.0e-2
max_grad_norm: 0.5
fused: False
eight_bit: False

# logging, eval, & checkpointing
use_wandb: True
wandb_project: "cramming-test"
wandb_watch: True
log_interval: 25
val_interval: 1000
save_interval: 1000
save_dir: "/content/drive/My Drive/cramBERT/model_checkpoints/"